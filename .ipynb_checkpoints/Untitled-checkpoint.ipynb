{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722ab463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-macosx_10_15_x86_64.whl (46.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /opt/anaconda3/lib/python3.9/site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97937bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIVE FEED (using opencv)\n",
    "\n",
    "# import\n",
    "import cv2\n",
    "\n",
    "# access camera - returns object cap\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# read/show frames from camera\n",
    "    # this line will only capture single frame,\n",
    "    # we need a loop\n",
    "    # _, frame = cap.read()\n",
    "    \n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    cv2.imshow('Live',frame)\n",
    "    # waitKey(x) func allows users to display a window,\n",
    "    # for x miliseconds or until any key is pressed\n",
    "    if cv2.waitKey(1) == 27: #27 is ascii ESC\n",
    "        break\n",
    "        \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02746a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement mediapipe-silicon (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for mediapipe-silicon\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# HAND TRACKING (using mediapipe)\n",
    "\n",
    "!pip install mediapipe-silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad168b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.9.0.1-cp39-cp39-macosx_10_15_x86_64.whl (35.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from mediapipe) (21.4.0)\n",
      "Collecting protobuf<4,>=3.11\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-macosx_10_9_x86_64.whl (982 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m982.8/982.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from mediapipe) (1.21.5)\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.6.0.66-cp36-abi3-macosx_10_15_x86_64.whl (56.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.9/site-packages (from mediapipe) (3.5.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->mediapipe) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: flatbuffers, protobuf, opencv-contrib-python, absl-py, mediapipe\n",
      "Successfully installed absl-py-1.3.0 flatbuffers-22.12.6 mediapipe-0.9.0.1 opencv-contrib-python-4.6.0.66 protobuf-3.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745ba1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand detection model works on ssd algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dab0b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "# Load Model\n",
    "hands = mp.solutions.hands\n",
    "# =1 makes it find only a single hand if there are multiple hands on cam\n",
    "hand_landmark = hands.Hands(max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1434c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# draw is provided by mediapipe\n",
    "draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read() # BGR(is what we got) we need RGB\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # op contains everything about hand\n",
    "    op = hand_landmark.process(rgb)\n",
    "    \n",
    "    # if there is a hand in the image op.multi... will contain something,\n",
    "    # else it will be none\n",
    "    if op.multi_hand_landmarks:\n",
    "        for all_landmarks in op.multi_hand_landmarks:\n",
    "            draw.draw_landmarks(frame, all_landmarks, hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    cv2.imshow('Live',frame)\n",
    "    if cv2.waitKey(1) == 27: #27 is ascii ESC\n",
    "        break\n",
    "        \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19cbb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAWING\n",
    "# 1. locate index\n",
    "# 2. draw line\n",
    "\n",
    "# camera frame resolution\n",
    "frame_shape = (1080, 1920, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8847161b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kw/hyfkh0bs5ln0y0bmt0kxqy9r0000gn/T/ipykernel_37264/1700868340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# op contains everything about hand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhand_landmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# if there is a hand in the image op.multi... will contain something,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/mediapipe/python/solutions/hands.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \"\"\"\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/mediapipe/python/solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;31m# output stream names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# importing numpy\n",
    "import numpy as np\n",
    "\n",
    "prevxy = None\n",
    "# mask is permanent values\n",
    "#mask = np.zeros(frame_shape, dtype='uint8') # to permanently draw, by default float, we tell it to do in int\n",
    "colour = (123,34,90)\n",
    "thickness = 4\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# draw is provided by mediapipe\n",
    "draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    # this frame changes after each iteration (temp frame)\n",
    "    _, frame = cap.read() # BGR(is what we got) we need RGB\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # op contains everything about hand\n",
    "    op = hand_landmark.process(rgb)\n",
    "    \n",
    "    # if there is a hand in the image op.multi... will contain something,\n",
    "    # else it will be none\n",
    "    if op.multi_hand_landmarks:\n",
    "        for all_landmarks in op.multi_hand_landmarks:\n",
    "            draw.draw_landmarks(frame, all_landmarks, hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # multiplying it because landmark gives in range b/w [0and1] so we multiply to get actual \n",
    "            x = int(all_landmarks.landmark[8].x * frame_shape[1])\n",
    "            y = int(all_landmarks.landmark[8].y * frame_shape[0])        \n",
    "    \n",
    "            if prevxy != None:\n",
    "                # draw stuff\n",
    "                # mask is nothing but frame but since it will change on every iteration we store it outside\n",
    "                cv2.line(mask, prevxy, (x, y), colour, thickness)\n",
    "            prevxy = (x, y)    \n",
    "            \n",
    "    # Merge frame and mask\n",
    "    # where mask has values it will accept it, or else take values of that loc from frame\n",
    "    frame = np.where(mask, mask, frame)\n",
    "    \n",
    "    cv2.imshow('Live', frame)\n",
    "    if cv2.waitKey(1) == 27: #27 is ascii ESC\n",
    "        break\n",
    "        \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29118040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERASER\n",
    "\n",
    "import cv2\n",
    "# importing numpy\n",
    "import numpy as np\n",
    "\n",
    "prevxy = None\n",
    "# mask is permanent values\n",
    "# mask = np.zeros(frame_shape, dtype='uint8') # to permanently draw, by default float, we tell it to do in int\n",
    "colour = (123,34,90)\n",
    "thickness = 4\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# draw is provided by mediapipe\n",
    "draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    # this frame changes after each iteration (temp frame)\n",
    "    _, frame = cap.read() # BGR(is what we got) we need RGB\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # op contains everything about hand\n",
    "    op = hand_landmark.process(rgb)\n",
    "    \n",
    "    # if there is a hand in the image op.multi... will contain something,\n",
    "    # else it will be none\n",
    "    if op.multi_hand_landmarks:\n",
    "        for all_landmarks in op.multi_hand_landmarks:\n",
    "            draw.draw_landmarks(frame, all_landmarks, hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # index finger location\n",
    "            # multiplying it because landmark gives in range b/w [0and1] so we multiply to get actual \n",
    "            x = int(all_landmarks.landmark[8].x * frame_shape[1])\n",
    "            y = int(all_landmarks.landmark[8].y * frame_shape[0])        \n",
    "    \n",
    "            cv2.circle(frame, (x, y), 30, (0,0,0), -1) # -1 means fill \n",
    "            cv2.circle(mask, (x, y), 30, (0,0,0), -1) # -1 means fill \n",
    "            \n",
    "    # Merge frame and mask\n",
    "    # where mask has values it will accept it, or else take values of that loc from frame\n",
    "    frame = np.where(mask, mask, frame)\n",
    "    \n",
    "    cv2.imshow('Live', frame)\n",
    "    if cv2.waitKey(1) == 27: #27 is ascii ESC\n",
    "        break\n",
    "        \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cc4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
